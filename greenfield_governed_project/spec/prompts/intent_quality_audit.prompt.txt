You are an LLM operating on a governed repository.

GOAL: Run the REQUIRED audits for intent <INTENT_ID> end-to-end and capture evidence:
1) Governance intent audit (`npm run audit:intent -- --intent-id <INTENT_ID>`)
2) Repository guardrails (`npm run guardrails`)
2) Audits implementation quality by inspecting the actual repo code
3) Produces a machine-readable quality audit report with 10 improvements ranked by ROI (0–10), each with effort + risk
4) Saves the report under `status/audit/<INTENT_ID>/runs/<run_id>/quality_audit.json`
5) Ensures portal views will show it after refresh (run generate/guardrails)

Hard rules:
- Never hand-edit any `.md` files (generated).
- Only edit canonical sources: `spec/intents/*.json`, `spec/tasks/*.json`, `spec/requirements/**.json`, code under `apps/**`, `packages/**`, `scripts/**`, `tools/**`, `pipeline/**`.
- The audit report must be JSON (not Markdown).

First ask me these questions (one at a time, max 8 total):
1) What is `<INTENT_ID>`?
2) What is `<run_id>`? (If I don’t care, propose a `run_id` like `YYYYMMDD_HHMMSS`.)
3) Which code areas are in-scope for the quality audit? (default: everything under `pipeline/`, `apps/portal/`, `scripts/`, `tools/`)
4) Any constraints? (time budget, “no refactors”, performance vs correctness, etc.)

Then execute this process:

A) REQUIRED audits + evidence (must pass before quality audit)
1) Run intent audit and capture evidence:
   `node tools/evidence/record_run.mjs --intent-id <INTENT_ID> --out status/audit/<INTENT_ID>/runs/<run_id>/audit/run.json --artefact status/audit/<INTENT_ID>/runs/<run_id>/audit/audit_report.json -- npm run audit:intent -- --intent-id <INTENT_ID> --out status/audit/<INTENT_ID>/runs/<run_id>/audit/audit_report.json`
2) Run guardrails and capture evidence:
   `node tools/evidence/record_run.mjs --intent-id <INTENT_ID> --out status/audit/<INTENT_ID>/runs/<run_id>/guardrails/run.json -- npm run guardrails`
3) If either fails, stop and list the exact required fixes; do not proceed until both pass.
4) Regression guardrail check for new requirements:
   - For any `REQ-*` requirement created/touched by this intent, confirm it includes `guardrails:req_tag_enforced_on_done` (automation enforces this via `npm run guardrails`).

B) Quality audit (inspect the real code)
1) Read the intent spec: `spec/intents/<INTENT_ID>.json` and all referenced task specs `spec/tasks/<TASK_ID>.json`.
2) Inspect actual implementation files referenced by deliverables plus any obvious adjacent code.
   - Treat `deliverables[]` + `subtasks[]` as the authoritative scope and check whether they are concrete enough.
3) Evaluate against these categories:
   - Correctness / trust-first behavior risks
   - Maintainability / clarity / cohesion
   - Testability (where tests should exist, even if none yet)
   - Performance + scalability bottlenecks
   - Security / safety (e.g., path handling, command execution, input validation)
   - Observability (logging quality, evidence capture usefulness)
4) Produce exactly 10 improvement items, ranked by ROI (0–10). Each item MUST include:
   - `id` (e.g. `IMP-001`)
   - `title`
   - `area` (e.g. `pipeline`, `portal`, `guardrails`, `generation`, `workflows`)
   - `roi_0_to_10` (number)
   - `effort` (`S`|`M`|`L`)
   - `risk` (`low`|`medium`|`high`)
   - `why` (1–3 sentences)
   - `proposed_changes` (bullet strings; concrete file/symbol pointers)

Quality bar (scope hygiene):
- If a deliverable path is only a top-level directory (e.g. `pipeline/`), call it out as a scope smell and propose specific file-level deliverables.

C) Write the report JSON (single file)
Create `status/audit/<INTENT_ID>/runs/<run_id>/quality_audit.json` with this schema:
{
  "type": "intent_quality_audit_report",
  "schema_version": 1,
  "intent_id": "<INTENT_ID>",
  "run_id": "<run_id>",
  "timestamp": "YYYY-MM-DDTHH:MM:SSZ",
  "scope": {
    "areas_in_scope": ["pipeline", "portal", "scripts", "tools"]
  },
  "audit_inputs": {
    "intent_spec_path": "spec/intents/<INTENT_ID>.json",
    "task_spec_paths": ["spec/tasks/TASK-...json"],
    "code_paths_reviewed": ["pipeline/src/...", "scripts/..."]
  },
  "summary": {
    "overall_health_0_to_10": 0,
    "top_risks": ["..."],
    "top_quick_wins": ["..."]
  },
  "improvements": [ /* exactly 10 items, sorted by roi desc */ ]
}

D) Validate + refresh surfaces
1) Run: `npm run generate`
2) Run: `npm run guardrails`
3) Tell me where the report file is and confirm it will appear in the portal after pressing Refresh on the intent page.

Output requirements (in chat):
- Show me only:
  - the audit evidence path: `status/audit/<INTENT_ID>/runs/<run_id>/audit/run.json`
  - the audit report path: `status/audit/<INTENT_ID>/runs/<run_id>/audit/audit_report.json`
  - the guardrails evidence path: `status/audit/<INTENT_ID>/runs/<run_id>/guardrails/run.json`
  - the quality report path: `status/audit/<INTENT_ID>/runs/<run_id>/quality_audit.json`
  - the top 3 improvement titles + ROI/effort/risk
Do not paste the entire JSON into chat.
