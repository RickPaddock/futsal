You are an LLM operating on a governed repository with local filesystem + command access.

DO NOT ask the human any questions. Do NOT request pasted files. Open/read everything yourself.

GOAL: Close intent `<INTENT_ID>` end-to-end, only if required audit outputs exist and passed.

Hard rules:
- Never hand-edit any `.md` files (generated).
- Edit only canonical sources: `spec/intents/*.json`, `spec/tasks/*.json`, `spec/requirements/**.json`, plus code.
- Evidence outputs MUST be JSON under `status/audit/<INTENT_ID>/runs/<run_id>/`.

CONTEXT (provided by the caller; do not ask for these):
- `intent_id`: `<INTENT_ID>`
- `run_id`: `<run_id>`
- `closed_date`: `<closed_date>` (YYYY-MM-DD)

Sanity check:
- If `<INTENT_ID>` / `<run_id>` / `<closed_date>` still contains angle-bracket placeholder markers, STOP: this prompt was not rendered with real values.

Outputs that MUST already exist before closing:
- `status/audit/<INTENT_ID>/runs/<run_id>/audit/run.json` (exit_code must be 0)
- `status/audit/<INTENT_ID>/runs/<run_id>/audit/audit_report.json` (`errors` must be empty)
- `status/audit/<INTENT_ID>/runs/<run_id>/guardrails/run.json` (exit_code must be 0)
- `status/audit/<INTENT_ID>/runs/<run_id>/quality_audit.json` (required for close)
- For EACH task in `spec/intents/<INTENT_ID>.json` â†’ `task_ids_planned[]`:
   - `status/audit/<INTENT_ID>/runs/<run_id>/tasks/TASK_ID/quality_audit.json` (must pass functional + non-functional gates)

Then execute this process in order:

A) Preflight (do not modify anything yet)
1) Open `spec/intents/<INTENT_ID>.json` and confirm:
   - `status` is `todo` (must not be `draft`)
   - `task_ids_planned[]` is non-empty
   - `runbooks` section exists with explicit decision (`none` | `create` | `update`) and non-empty `notes`
     - If `decision` is `create` or `update`, `runbooks.paths_mdt[]` lists impacted `spec/md/docs/runbooks/*.mdt` templates
   - `close_gate[]` includes `npm run audit:intent -- --intent-id <INTENT_ID>`
2) For each task in `task_ids_planned[]`, open `spec/tasks/TASK_ID.json`.
   - If any task is `status: todo`, confirm it has `deliverables[]` and at least one deliverable.
   - If deliverable paths are vague (directory-only), improve them before closing.
3) If any task/subtask indicates a new requirement (`REQ-*`), confirm that requirement exists in `spec/requirements/areas/*.json` with `tracking.implementation: "todo"`.
   - If missing, add it to the appropriate area file (status must be `draft`, tracking.implementation must be `todo`).
   - Ensure `guardrails[]` is non-empty and includes `guardrails:req_tag_enforced_on_done` (regression guardrail enforced by automation).

B) Required audits must already be completed (and evidenced)
1) Confirm these evidence files exist (they must come from the Audit prompt run):
   - `status/audit/<INTENT_ID>/runs/<run_id>/audit/run.json` (exit_code 0)
   - `status/audit/<INTENT_ID>/runs/<run_id>/audit/audit_report.json` (errors empty)
   - `status/audit/<INTENT_ID>/runs/<run_id>/guardrails/run.json` (exit_code 0)
   - `status/audit/<INTENT_ID>/runs/<run_id>/quality_audit.json`
   - `status/audit/<INTENT_ID>/runs/<run_id>/tasks/TASK_ID/quality_audit.json` for every planned task
2) Open the evidence JSON and confirm:
   - All exit codes are 0
   - `audit_report.json` has `errors: []`
   - Every task quality audit has:
     - `gate.status: "pass"` and `gate.blockers: []`
     - `functional.status: "pass"`
     - `nonfunctional.overall_status: "pass"` (and each category status is `"pass"`)
3) If any are missing or failing, STOP. Run the Audit prompt first and fix issues until audits pass.

D) Close intent (only if audits pass)
1) Dry-run close check (no writes):
   `npm run intent:close -- --intent-id <INTENT_ID> --run-id <run_id> --require-quality-audit --closed-date <closed_date>`
   - If this fails (e.g. missing `REQ:` code references), stop and list exact fixes.
2) Apply close:
   `npm run intent:close -- --intent-id <INTENT_ID> --run-id <run_id> --require-quality-audit --closed-date <closed_date> --apply`
This MUST:
- Set `spec/intents/<INTENT_ID>.json` to `status: "closed"` and set `closed_date`
- For any `REQ-*` created by this intent, set `tracking.implementation: "done"` ONLY when code exists and references `REQ: REQ-*`

E) Refresh derived surfaces + validate
1) `npm run generate`
2) `npm run guardrails`

F) Evidence (MUST)
Record the close apply as evidence (and attach the audit artefacts you created above; missing artefacts are ignored):
`node tools/evidence/record_run.mjs --intent-id <INTENT_ID> --out status/audit/<INTENT_ID>/runs/<run_id>/close/run.json --artefact status/audit/<INTENT_ID>/runs/<run_id>/audit/audit_report.json --artefact status/audit/<INTENT_ID>/runs/<run_id>/quality_audit.json npm run intent:close -- --intent-id <INTENT_ID> --run-id <run_id> --require-quality-audit --closed-date <closed_date> --apply`

Output requirements (in chat):
- Only show:
  - The evidence file paths used/written:
    - `status/audit/<INTENT_ID>/runs/<run_id>/audit/run.json`
    - `status/audit/<INTENT_ID>/runs/<run_id>/audit/audit_report.json`
    - `status/audit/<INTENT_ID>/runs/<run_id>/guardrails/run.json`
    - `status/audit/<INTENT_ID>/runs/<run_id>/close/run.json`
  - Confirmation that intent status is now `closed` and will show in the portal after Refresh
Do not paste the full JSON reports into chat.
